{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Natural Language Processing of Disaster Tweets\n\nTwitter has become an important source of real-time information, given its ease of use and quick access. Users can quickly send out short 'tweets' to update on disasters that they are witnessing, which has led to many companies and monitoring agencies using it as a source of real-time information. However, since the vocabulary used in describing disasters is also used in unrelated contexts or as literary devices, these agencies need a way of checking if a tweet containing these words is describing an actual disaster.\n","metadata":{}},{"cell_type":"markdown","source":"# Exploring the Data\n\nWe have a training and testing dataset consisting of tweets, some of which are related to an actual disaster. The training set has 7503 datapoints, and the testing set has 3243. The features present in the training set are:\n\n1. keywords\n + a particular keyword from the tweet (may be blank)\n2. location\n + the location the tweet was sent from (may be blank)\n3. text\n + the text of the tweet\n4. target\n + this denotes whether a tweet is about a real disaster (1) or not (0)\n \nWe will begin by exploring our data and performing the eye-test on the contents of the tweets. We will also be performing n-gram analysis on our tweets after doing some preprocessing.","metadata":{}},{"cell_type":"code","source":"# import necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.preprocessing import OrdinalEncoder\n\nfrom nltk.tokenize import *\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nfrom nltk.probability import FreqDist\n\nimport re\nimport string\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 100000)\npd.set_option('display.max_colwidth', None)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:48:36.811934Z","iopub.execute_input":"2022-08-03T10:48:36.813109Z","iopub.status.idle":"2022-08-03T10:48:38.536559Z","shell.execute_reply.started":"2022-08-03T10:48:36.812970Z","shell.execute_reply":"2022-08-03T10:48:38.535161Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# nltk data used for preprocessing\n\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:48:38.746402Z","iopub.execute_input":"2022-08-03T10:48:38.746794Z","iopub.status.idle":"2022-08-03T10:48:39.377372Z","shell.execute_reply.started":"2022-08-03T10:48:38.746761Z","shell.execute_reply":"2022-08-03T10:48:39.376214Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:48:42.476392Z","iopub.execute_input":"2022-08-03T10:48:42.476986Z","iopub.status.idle":"2022-08-03T10:48:42.535217Z","shell.execute_reply.started":"2022-08-03T10:48:42.476919Z","shell.execute_reply":"2022-08-03T10:48:42.534241Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:01:38.165518Z","iopub.execute_input":"2022-08-03T10:01:38.165962Z","iopub.status.idle":"2022-08-03T10:01:38.198992Z","shell.execute_reply.started":"2022-08-03T10:01:38.165928Z","shell.execute_reply":"2022-08-03T10:01:38.197330Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"NO_TRAIN_NON_DISASTER = train['target'].value_counts()[0]\nNO_TRAIN_DISASTER = train['target'].value_counts()[1]\n","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:48:44.695997Z","iopub.execute_input":"2022-08-03T10:48:44.696977Z","iopub.status.idle":"2022-08-03T10:48:44.713887Z","shell.execute_reply.started":"2022-08-03T10:48:44.696930Z","shell.execute_reply":"2022-08-03T10:48:44.712908Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"We note that about 25% of the data contains missing location data, while only about 0.6% is missing keyword data.\n\nSince location data is not something that can be immediately derived without the use of external datasets, we decide to set the missing values to a placeholder value of 'no_location'.\n\nFor the keyword columns, one possible way of filling in the missing values is by parsing the grammatical structure of the tweet and searching for the grammatical 'object' and using it as the keyword. However, there is no guarantee on the correct spelling of the words in the tweet, which would add unnecessary outliers to our keyword data. (e.g. 'goal' is spelt as goooooooaaaaaalll' in tweet 28). Similarly, the keyword in some tweets do not necessarily correspond to its object. For example, tweet 37 ('No way...I can't eat that shit') has 'shit' as an object, though one would likely agree that 'eat' would likely be the more useful/descriptive keyword here.\n\nWe will hence fill in the remaining missing values with a placeholder of 'no_keyword'.","metadata":{}},{"cell_type":"code","source":"# fill in nas with no_location, no_keyword\n\ntrain_processed = train.fillna(value={'keyword': 'no_keyword', 'location': 'no_location'}, )","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:48:46.403953Z","iopub.execute_input":"2022-08-03T10:48:46.405017Z","iopub.status.idle":"2022-08-03T10:48:46.412374Z","shell.execute_reply.started":"2022-08-03T10:48:46.404978Z","shell.execute_reply":"2022-08-03T10:48:46.411337Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_processed.info()","metadata":{"execution":{"iopub.status.busy":"2022-08-02T18:48:04.213274Z","iopub.execute_input":"2022-08-02T18:48:04.214224Z","iopub.status.idle":"2022-08-02T18:48:04.230408Z","shell.execute_reply.started":"2022-08-02T18:48:04.214177Z","shell.execute_reply":"2022-08-02T18:48:04.228706Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"We want to look through the top keywords and locations from our dataset.","metadata":{}},{"cell_type":"code","source":"train_processed[train_processed['target']==1]['keyword'].value_counts()[:20].plot.barh()\nplt.title('Top 20 keyword values (Disasters)')","metadata":{"execution":{"iopub.status.busy":"2022-08-02T18:48:04.521895Z","iopub.execute_input":"2022-08-02T18:48:04.522668Z","iopub.status.idle":"2022-08-02T18:48:04.838154Z","shell.execute_reply.started":"2022-08-02T18:48:04.522599Z","shell.execute_reply":"2022-08-02T18:48:04.836543Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_processed[train_processed['target']==0]['keyword'].value_counts()[:20].plot.barh()\nplt.title('Top 20 keyword values (Not Disasters)')","metadata":{"execution":{"iopub.status.busy":"2022-08-02T18:48:04.841024Z","iopub.execute_input":"2022-08-02T18:48:04.841808Z","iopub.status.idle":"2022-08-02T18:48:05.093139Z","shell.execute_reply.started":"2022-08-02T18:48:04.841747Z","shell.execute_reply":"2022-08-02T18:48:05.091847Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_processed[(train_processed['target'] == 0) & (train_processed['keyword'] == 'blizzard')].head()","metadata":{"execution":{"iopub.status.busy":"2022-08-02T18:48:05.095439Z","iopub.execute_input":"2022-08-02T18:48:05.096311Z","iopub.status.idle":"2022-08-02T18:48:05.116154Z","shell.execute_reply.started":"2022-08-02T18:48:05.096259Z","shell.execute_reply":"2022-08-02T18:48:05.115199Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Surprising, words like 'blizzard' and 'body bags' are part of the top 20 keyword values for non-disaster related tweets. To get some context on these types of tweets, we looked at several examples. We note that tweets which had 'blizzard', for example, was used to refer to the company Blizzard. This informs us that we cannot just use a single word as an indicator for whether a tweet is about a disaster or not. ","metadata":{}},{"cell_type":"code","source":"train_processed[train_processed['target']==1]['location'].value_counts()[1:11].plot.barh()\nplt.title('Top 10 location values (Disasters)\\n Excludes no_location')","metadata":{"execution":{"iopub.status.busy":"2022-08-02T18:48:05.353495Z","iopub.execute_input":"2022-08-02T18:48:05.354412Z","iopub.status.idle":"2022-08-02T18:48:05.590454Z","shell.execute_reply.started":"2022-08-02T18:48:05.354364Z","shell.execute_reply":"2022-08-02T18:48:05.589200Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_processed[train_processed['target']==0]['location'].value_counts()[1:11].plot.barh()\nplt.title('Top 10 location values (Not Disasters)\\nExcludes no_location')","metadata":{"execution":{"iopub.status.busy":"2022-08-02T18:48:05.592568Z","iopub.execute_input":"2022-08-02T18:48:05.592944Z","iopub.status.idle":"2022-08-02T18:48:05.821956Z","shell.execute_reply.started":"2022-08-02T18:48:05.592911Z","shell.execute_reply":"2022-08-02T18:48:05.820796Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"temp1 = train_processed[train_processed['target']==1]['location'].value_counts()[1:11]\ntemp2 = train_processed[train_processed['target']==0]['location'].value_counts().filter(items=list(temp1.index), axis=0)\n\ntemp = pd.DataFrame({'disaster': temp1, 'non_disaster': temp2})\ntemp = temp.div(temp.sum(axis=1), axis=0)\n\ntemp.plot.barh()\n\nplt.title('Location vs. whether a tweet is related to disaster or non-disaster')\nplt.xlabel('Percentage of total tweets')","metadata":{"execution":{"iopub.status.busy":"2022-08-02T18:48:05.823344Z","iopub.execute_input":"2022-08-02T18:48:05.823765Z","iopub.status.idle":"2022-08-02T18:48:06.115037Z","shell.execute_reply.started":"2022-08-02T18:48:05.823731Z","shell.execute_reply":"2022-08-02T18:48:06.113720Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"We see that tweets from certain locations have a higher proportions of tweets that are related to disasters than non-disasters. In particular, Mumbai, India, and Nigeria exhibit such behaviour. Locations like New York, however, are more likely to have non-disaster tweets than disaster tweets.","metadata":{}},{"cell_type":"markdown","source":"# Data Preprocessing\n\nNext, we will begin to preprocess our data and clean it up in preparation for feature learning. We will use the basic steps shared by in this [intro notebook by Parul Pandey](https://www.kaggle.com/parulpandey) to guide our preprocessing.\n\nThe techniques include:\n- Lower casing\n- Tokenisation\n- Stop words removal\n- Stemming\n- Lemmatization\n\n[API reference](https://www.nltk.org/api/nltk.html) and [guide to using with Pandas](https://www.kirenz.com/post/2021-12-11-text-mining-and-sentiment-analysis-with-nltk-and-pandas-in-python/text-mining-and-sentiment-analysis-with-nltk-and-pandas-in-python/)","metadata":{}},{"cell_type":"markdown","source":"## Lower-casing","metadata":{}},{"cell_type":"code","source":"# lower casing all texts\n\ntrain_processed['cleaned_text'] = train_processed['text'].str.lower()","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:49:16.571022Z","iopub.execute_input":"2022-08-03T10:49:16.571432Z","iopub.status.idle":"2022-08-03T10:49:16.582790Z","shell.execute_reply.started":"2022-08-03T10:49:16.571399Z","shell.execute_reply":"2022-08-03T10:49:16.581629Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_processed.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-08-03T10:49:17.453076Z","iopub.execute_input":"2022-08-03T10:49:17.454042Z","iopub.status.idle":"2022-08-03T10:49:17.471615Z","shell.execute_reply.started":"2022-08-03T10:49:17.453990Z","shell.execute_reply":"2022-08-03T10:49:17.470411Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Remove URLs and hashtags\n\nWe note from analysis that we did retroactively that there are URLs present in many tweets that link to external sites. As such, we will attempt to remove them so that common strings that are useless for us like 'http' will not show up in our data.\n\nSimilarly, hashtags are common in tweets but do not have any special meaning for our use. Hence, we will remove them.","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    result = re.sub(r\"http\\S+\", \"\", text)\n    result = re.sub(r\"https\\S+\", \"\", result)\n    result = re.sub(r\"#\", \"\", result)\n    return result\n\ntrain_processed['cleaned_text'] = train_processed['cleaned_text'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:49:18.619495Z","iopub.execute_input":"2022-08-03T10:49:18.620174Z","iopub.status.idle":"2022-08-03T10:49:18.663209Z","shell.execute_reply.started":"2022-08-03T10:49:18.620127Z","shell.execute_reply":"2022-08-03T10:49:18.662006Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Tokenisation","metadata":{}},{"cell_type":"code","source":"# Tokenisation via nltk\n\ntrain_processed['tokens'] = train_processed['cleaned_text'].apply(word_tokenize)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:49:19.098902Z","iopub.execute_input":"2022-08-03T10:49:19.099343Z","iopub.status.idle":"2022-08-03T10:49:20.561229Z","shell.execute_reply.started":"2022-08-03T10:49:19.099298Z","shell.execute_reply":"2022-08-03T10:49:20.560122Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_processed.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-08-03T10:49:20.563578Z","iopub.execute_input":"2022-08-03T10:49:20.564080Z","iopub.status.idle":"2022-08-03T10:49:20.581204Z","shell.execute_reply.started":"2022-08-03T10:49:20.564016Z","shell.execute_reply":"2022-08-03T10:49:20.579795Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Stop-words removal","metadata":{}},{"cell_type":"code","source":"# remove stop words using nltk\n\nstopwords = nltk.corpus.stopwords.words('english')\n\ntrain_processed['tokens'] = train_processed['tokens'].apply(lambda x: [item for item in x if item not in stopwords])","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:49:20.583413Z","iopub.execute_input":"2022-08-03T10:49:20.583876Z","iopub.status.idle":"2022-08-03T10:49:20.921948Z","shell.execute_reply.started":"2022-08-03T10:49:20.583833Z","shell.execute_reply":"2022-08-03T10:49:20.920637Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_processed.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:49:20.924095Z","iopub.execute_input":"2022-08-03T10:49:20.924415Z","iopub.status.idle":"2022-08-03T10:49:20.939720Z","shell.execute_reply.started":"2022-08-03T10:49:20.924385Z","shell.execute_reply":"2022-08-03T10:49:20.938478Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Remove punctuation tokens","metadata":{}},{"cell_type":"code","source":"# remove punctuation tokens\n\npuncs = [char for char in string.punctuation]\n\ntrain_processed['tokens'] = train_processed['tokens'].apply(lambda x: [item for item in x if item not in puncs])","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:49:20.941425Z","iopub.execute_input":"2022-08-03T10:49:20.941874Z","iopub.status.idle":"2022-08-03T10:49:21.012468Z","shell.execute_reply.started":"2022-08-03T10:49:20.941831Z","shell.execute_reply":"2022-08-03T10:49:21.011538Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_processed.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:49:21.108308Z","iopub.execute_input":"2022-08-03T10:49:21.109028Z","iopub.status.idle":"2022-08-03T10:49:21.125846Z","shell.execute_reply.started":"2022-08-03T10:49:21.108984Z","shell.execute_reply":"2022-08-03T10:49:21.124615Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Lemmatize tokens","metadata":{}},{"cell_type":"code","source":"# lemmatise using wordnetlemmatise\n\nwnl = WordNetLemmatizer()\n\ntrain_processed['tokens_lem'] = train_processed['tokens'].apply(lambda x: [wnl.lemmatize(item) for item in x])\n","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:49:22.118312Z","iopub.execute_input":"2022-08-03T10:49:22.118678Z","iopub.status.idle":"2022-08-03T10:49:24.428268Z","shell.execute_reply.started":"2022-08-03T10:49:22.118647Z","shell.execute_reply":"2022-08-03T10:49:24.427107Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_processed.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-08-03T10:49:24.430222Z","iopub.execute_input":"2022-08-03T10:49:24.430758Z","iopub.status.idle":"2022-08-03T10:49:24.450249Z","shell.execute_reply.started":"2022-08-03T10:49:24.430711Z","shell.execute_reply":"2022-08-03T10:49:24.449004Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Count Vectorise our dataset","metadata":{}},{"cell_type":"code","source":"# make the tokens into a string for our sklearn countvectoriser function\n\ndef combine_tokens(tokens):\n    string = ''\n    for token in tokens:\n        string += ' '\n        string += token\n    return string\n\ntrain_processed['tokens_string'] = train_processed['tokens_lem'].apply(combine_tokens)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:49:26.379207Z","iopub.execute_input":"2022-08-03T10:49:26.380284Z","iopub.status.idle":"2022-08-03T10:49:26.400067Z","shell.execute_reply.started":"2022-08-03T10:49:26.380246Z","shell.execute_reply":"2022-08-03T10:49:26.398826Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_processed.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:49:27.410971Z","iopub.execute_input":"2022-08-03T10:49:27.411735Z","iopub.status.idle":"2022-08-03T10:49:27.432617Z","shell.execute_reply.started":"2022-08-03T10:49:27.411688Z","shell.execute_reply":"2022-08-03T10:49:27.431637Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Encode categorical variables","metadata":{}},{"cell_type":"code","source":"# encode using sklearn's OrdinalEncoder\n\n# these encoders will be used for the training, validation, and test sets.\noe_location = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=6969)\noe_keyword = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=6969)\n\noe_location.fit(train_processed['location'].to_frame())\noe_keyword.fit(train_processed['keyword'].to_frame())\n\ntrain_processed['location'] = oe_location.transform(train_processed['location'].to_frame())\ntrain_processed['keyword'] = oe_keyword.transform(train_processed['keyword'].to_frame())","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:49:29.165042Z","iopub.execute_input":"2022-08-03T10:49:29.165911Z","iopub.status.idle":"2022-08-03T10:49:29.198845Z","shell.execute_reply.started":"2022-08-03T10:49:29.165875Z","shell.execute_reply":"2022-08-03T10:49:29.197892Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"train_processed.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:49:30.396077Z","iopub.execute_input":"2022-08-03T10:49:30.396817Z","iopub.status.idle":"2022-08-03T10:49:30.416983Z","shell.execute_reply.started":"2022-08-03T10:49:30.396779Z","shell.execute_reply":"2022-08-03T10:49:30.416016Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Unigrams (1-grams)","metadata":{}},{"cell_type":"code","source":"# Get counts of each unique token for each corpus.\n\nv1 = CountVectorizer()\n\nX1 = v1.fit_transform(train_processed['tokens_string'])\nX1_matrix = X1.toarray()\nX1_features = v1.get_feature_names_out()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-08-02T18:46:59.877363Z","iopub.execute_input":"2022-08-02T18:46:59.878655Z","iopub.status.idle":"2022-08-02T18:47:00.233460Z","shell.execute_reply.started":"2022-08-02T18:46:59.878580Z","shell.execute_reply":"2022-08-02T18:47:00.232582Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"unigrams_df = pd.DataFrame(X1_matrix, columns=list(X1_features))","metadata":{"execution":{"iopub.status.busy":"2022-08-02T18:47:00.235095Z","iopub.execute_input":"2022-08-02T18:47:00.235726Z","iopub.status.idle":"2022-08-02T18:47:00.243143Z","shell.execute_reply.started":"2022-08-02T18:47:00.235689Z","shell.execute_reply":"2022-08-02T18:47:00.241983Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# plot the top 10 most frequent unigrams\ntemp = unigrams_df.sum().sort_values(ascending=False)\n\ntemp[:10].plot.barh()\nplt.title('Top 10 most frequent unigrams')","metadata":{"execution":{"iopub.status.busy":"2022-08-02T18:47:00.251740Z","iopub.execute_input":"2022-08-02T18:47:00.252121Z","iopub.status.idle":"2022-08-02T18:47:00.851472Z","shell.execute_reply.started":"2022-08-02T18:47:00.252085Z","shell.execute_reply":"2022-08-02T18:47:00.850473Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Combine with training data\n\ntrain_unigram = train_processed.merge(unigrams_df, left_index=True, right_index=True, suffixes=['x', None])\n\ntrain_unigram.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-08-02T18:47:00.853309Z","iopub.execute_input":"2022-08-02T18:47:00.854551Z","iopub.status.idle":"2022-08-02T18:47:02.916955Z","shell.execute_reply.started":"2022-08-02T18:47:00.854500Z","shell.execute_reply":"2022-08-02T18:47:02.915746Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# check most common unigrams of disaster and non-disaster related tweets\n\ntemp = train_unigram.groupby('targetx').sum().drop('idx', axis=1)\nindexes = list(unigrams_df.sum().sort_values(ascending=False).index)[:10]\n\nt1 = temp.iloc[0, :].loc[indexes]\nt2 = temp.iloc[1, :].loc[indexes]\n\nt3 = pd.DataFrame({'disaster': t2, 'non_disaster': t1})\nt3['disaster'] = t3['disaster'].div(NO_TRAIN_DISASTER)\nt3['non_disaster'] = t3['non_disaster'].div(NO_TRAIN_NON_DISASTER)\n\nt3.plot.barh()\nplt.title('Unigrams vs frequency in tweets(split by disaster vs non_disaster tweets)')","metadata":{"execution":{"iopub.status.busy":"2022-08-02T18:47:02.918409Z","iopub.execute_input":"2022-08-02T18:47:02.919737Z","iopub.status.idle":"2022-08-02T18:47:06.080732Z","shell.execute_reply.started":"2022-08-02T18:47:02.919690Z","shell.execute_reply":"2022-08-02T18:47:06.079580Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"## Bigrams (2-grams)","metadata":{}},{"cell_type":"code","source":"# Get counts of each unique token for each corpus.\n\nv2 = CountVectorizer(ngram_range=(2, 2))\n\nX2 = v2.fit_transform(train_processed['tokens_string'])\nX2_matrix = X2.toarray()\nX2_features = v2.get_feature_names_out()","metadata":{"execution":{"iopub.status.busy":"2022-08-02T18:47:06.082231Z","iopub.execute_input":"2022-08-02T18:47:06.082619Z","iopub.status.idle":"2022-08-02T18:47:06.760131Z","shell.execute_reply.started":"2022-08-02T18:47:06.082567Z","shell.execute_reply":"2022-08-02T18:47:06.759213Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"bigrams_df = pd.DataFrame(X2_matrix, columns=list(X2_features))","metadata":{"execution":{"iopub.status.busy":"2022-08-02T18:47:06.761347Z","iopub.execute_input":"2022-08-02T18:47:06.762393Z","iopub.status.idle":"2022-08-02T18:47:06.777971Z","shell.execute_reply.started":"2022-08-02T18:47:06.762348Z","shell.execute_reply":"2022-08-02T18:47:06.776944Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# plot the top 10 most frequent bigrams\ntemp = bigrams_df.sum().sort_values(ascending=False)\n\ntemp[:10].plot.barh()\nplt.title('Top 10 most frequent bigrams')","metadata":{"execution":{"iopub.status.busy":"2022-08-02T18:47:06.779600Z","iopub.execute_input":"2022-08-02T18:47:06.780093Z","iopub.status.idle":"2022-08-02T18:47:08.312740Z","shell.execute_reply.started":"2022-08-02T18:47:06.780010Z","shell.execute_reply":"2022-08-02T18:47:08.311518Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Combine with training data\n\ntrain_bigram = train_processed.merge(bigrams_df, left_index=True, right_index=True, suffixes=['x', None])\n\ntrain_bigram.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-08-02T18:47:08.314150Z","iopub.execute_input":"2022-08-02T18:47:08.314626Z","iopub.status.idle":"2022-08-02T18:47:14.379295Z","shell.execute_reply.started":"2022-08-02T18:47:08.314559Z","shell.execute_reply":"2022-08-02T18:47:14.378158Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# check most common bigrams of disaster and non-disaster related tweets\n\ntemp = train_bigram.groupby('target').sum().drop('id', axis=1)\nindexes = list(bigrams_df.sum().sort_values(ascending=False).index)[:10]\nt1 = temp.iloc[0, :].loc[indexes]\nt2 = temp.iloc[1, :].loc[indexes]\n\nt3 = pd.DataFrame({'disaster': t2, 'non_disaster': t1})\nt3['disaster'] = t3['disaster'].div(NO_TRAIN_DISASTER)\nt3['non_disaster'] = t3['non_disaster'].div(NO_TRAIN_NON_DISASTER)\n\nt3.plot.barh()\nplt.title('Bigrams vs normalised frequency in tweets(split by disaster vs non_disaster tweets)')","metadata":{"execution":{"iopub.status.busy":"2022-08-02T18:47:14.380840Z","iopub.execute_input":"2022-08-02T18:47:14.381740Z","iopub.status.idle":"2022-08-02T18:47:29.678424Z","shell.execute_reply.started":"2022-08-02T18:47:14.381691Z","shell.execute_reply":"2022-08-02T18:47:29.677111Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"## Trigrams (3-grams)","metadata":{}},{"cell_type":"code","source":"# Get counts of each unique token for each corpus.\n\nv3 = CountVectorizer(ngram_range=(3, 3))\n\nX3 = v3.fit(train_processed['tokens_string']).transform(train_processed['tokens_string'])\nX3_matrix = X3.toarray()\nX3_features = v3.get_feature_names_out()","metadata":{"execution":{"iopub.status.busy":"2022-08-03T11:37:25.384938Z","iopub.execute_input":"2022-08-03T11:37:25.385675Z","iopub.status.idle":"2022-08-03T11:37:26.160914Z","shell.execute_reply.started":"2022-08-03T11:37:25.385632Z","shell.execute_reply":"2022-08-03T11:37:26.159218Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"trigrams_df = pd.DataFrame(X3_matrix, columns=list(X3_features))","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:49:47.034467Z","iopub.execute_input":"2022-08-03T10:49:47.034958Z","iopub.status.idle":"2022-08-03T10:49:47.054450Z","shell.execute_reply.started":"2022-08-03T10:49:47.034916Z","shell.execute_reply":"2022-08-03T10:49:47.053174Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# plot the top 10 most frequent trigrams\ntemp = trigrams_df.sum().sort_values(ascending=False)\n\ntemp[:10].plot.barh()\nplt.title('Top 10 most frequent trigrams')","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:49:52.040563Z","iopub.execute_input":"2022-08-03T10:49:52.041010Z","iopub.status.idle":"2022-08-03T10:49:53.482500Z","shell.execute_reply.started":"2022-08-03T10:49:52.040973Z","shell.execute_reply":"2022-08-03T10:49:53.481252Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Combine with training data\n\ntrain_trigram = train_processed.merge(pd.DataFrame(X3_matrix, columns=list(X3_features)), left_index=True, right_index=True, suffixes=['x', None])\n","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:50:08.791762Z","iopub.execute_input":"2022-08-03T10:50:08.792230Z","iopub.status.idle":"2022-08-03T10:50:14.350732Z","shell.execute_reply.started":"2022-08-03T10:50:08.792184Z","shell.execute_reply":"2022-08-03T10:50:14.349500Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# check most common bigrams of disaster and non-disaster related tweets\n\ntemp = train_trigram.groupby('target').sum().drop('id', axis=1)\nindexes = list(trigrams_df.sum().sort_values(ascending=False).index)[:10]\nt1 = temp.iloc[0, :].loc[indexes]\nt2 = temp.iloc[1, :].loc[indexes]\n\nt3 = pd.DataFrame({'disaster': t2, 'non_disaster': t1})\nt3['disaster'] = t3['disaster'].div(NO_TRAIN_DISASTER)\nt3['non_disaster'] = t3['non_disaster'].div(NO_TRAIN_NON_DISASTER)\n\nt3.plot.barh()\nplt.title('Trigrams vs normalised frequency in tweets(split by disaster vs non_disaster tweets)')","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:50:25.878412Z","iopub.execute_input":"2022-08-03T10:50:25.878820Z","iopub.status.idle":"2022-08-03T10:50:34.471842Z","shell.execute_reply.started":"2022-08-03T10:50:25.878786Z","shell.execute_reply":"2022-08-03T10:50:34.470364Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# Building a Trigram model\n\nWe choose a trigram model because of the top trigrams found in the dataset, the top 10 largely are related to disaster tweets -- more so than the unigram and bigrams. We do not go higher than this in order to maintain generality of our model.\n\n\nWe will attempt several methods.","metadata":{}},{"cell_type":"markdown","source":"## Prepare training and validation datasets","metadata":{}},{"cell_type":"code","source":"X = train_trigram.drop(['text', 'cleaned_text', 'tokens', 'tokens_lem', 'tokens_string', 'id', 'target'], axis=1)\ny = train_trigram.loc[:, 'target']","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:50:43.335502Z","iopub.execute_input":"2022-08-03T10:50:43.335905Z","iopub.status.idle":"2022-08-03T10:50:44.196329Z","shell.execute_reply.started":"2022-08-03T10:50:43.335865Z","shell.execute_reply":"2022-08-03T10:50:44.195125Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# garbage collection\n\ntrain_trigram = 0\ntrain = 0\nX3 = 0\nX3_matrix = 0\nX3_features = 0\ntemp = 0","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:50:44.384295Z","iopub.execute_input":"2022-08-03T10:50:44.385292Z","iopub.status.idle":"2022-08-03T10:50:44.392080Z","shell.execute_reply.started":"2022-08-03T10:50:44.385252Z","shell.execute_reply":"2022-08-03T10:50:44.391157Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX = 0\ny = 0","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:50:46.627618Z","iopub.execute_input":"2022-08-03T10:50:46.628827Z","iopub.status.idle":"2022-08-03T10:50:48.932722Z","shell.execute_reply.started":"2022-08-03T10:50:46.628773Z","shell.execute_reply":"2022-08-03T10:50:48.931789Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression\n\nWe will use a logistic regression classification model with cross validation (3-folds). We chose this model due to the data being exceptionally high dimensional, which would make a Tree-based model work extremely slow.Also, this works because all of our data has been converted into numerical data.","metadata":{}},{"cell_type":"code","source":"clf = LogisticRegressionCV(cv=3, random_state=42, max_iter=300).fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T10:54:26.013114Z","iopub.execute_input":"2022-08-03T10:54:26.013544Z","iopub.status.idle":"2022-08-03T10:57:20.489549Z","shell.execute_reply.started":"2022-08-03T10:54:26.013507Z","shell.execute_reply":"2022-08-03T10:57:20.487890Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# validation\n\nclf.score(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T11:02:23.572680Z","iopub.execute_input":"2022-08-03T11:02:23.573133Z","iopub.status.idle":"2022-08-03T11:02:24.265629Z","shell.execute_reply.started":"2022-08-03T11:02:23.573097Z","shell.execute_reply":"2022-08-03T11:02:24.264372Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# Creating Test Submission\n\nNow that we have a basic model - though admittedly the validation score of 0.66 is not great - we can prepare our testing file and use the model to predict the target values.","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('../input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-08-03T11:19:45.629228Z","iopub.execute_input":"2022-08-03T11:19:45.630497Z","iopub.status.idle":"2022-08-03T11:19:45.660331Z","shell.execute_reply.started":"2022-08-03T11:19:45.630445Z","shell.execute_reply":"2022-08-03T11:19:45.659180Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## Preparing Test Data\n\nTo make things more concise, we will create a simple preprocessing pipeline. After that, we will count vectorise the tokens and merge the count matrix with our data.","metadata":{}},{"cell_type":"code","source":"def preprocessingPipeline(data):\n    \n    processed = data.fillna(value={'keyword': 'no_keyword', 'location': 'no_location'}, )\n    \n    # lower casing all texts\n    \n    processed['cleaned_text'] = processed['text'].str.lower()\n    \n    # remove urls and hashtags\n    def clean_text(text):\n        result = re.sub(r\"http\\S+\", \"\", text)\n        result = re.sub(r\"https\\S+\", \"\", result)\n        result = re.sub(r\"#\", \"\", result)\n        return result\n\n    processed['cleaned_text'] = processed['cleaned_text'].apply(clean_text)\n    \n    # Tokenisation via nltk\n    \n    processed['tokens'] = processed['cleaned_text'].apply(word_tokenize)\n    \n    # remove stop words using nltk\n\n    stopwords = nltk.corpus.stopwords.words('english')\n    processed['tokens'] = processed['tokens'].apply(lambda x: [item for item in x if item not in stopwords])\n    \n    # remove punctuation tokens\n\n    puncs = [char for char in string.punctuation]\n    processed['tokens'] = processed['tokens'].apply(lambda x: [item for item in x if item not in puncs])\n    \n    # lemmatise using wordnetlemmatise\n\n    wnl = WordNetLemmatizer()\n    processed['tokens_lem'] = processed['tokens'].apply(lambda x: [wnl.lemmatize(item) for item in x])\n\n    # make the tokens into a string for our sklearn countvectoriser function\n\n    def combine_tokens(tokens):\n        string = ''\n        for token in tokens:\n            string += ' '\n            string += token\n        return string\n\n    processed['tokens_string'] = processed['tokens_lem'].apply(combine_tokens)\n    \n    # encode using training set trained OrdinalEncoders\n    \n    processed['location'] = oe_location.transform(processed['location'].to_frame())\n    processed['keyword'] = oe_keyword.transform(processed['keyword'].to_frame())\n    \n    return processed","metadata":{"execution":{"iopub.status.busy":"2022-08-03T11:19:50.037988Z","iopub.execute_input":"2022-08-03T11:19:50.038928Z","iopub.status.idle":"2022-08-03T11:19:50.056773Z","shell.execute_reply.started":"2022-08-03T11:19:50.038877Z","shell.execute_reply":"2022-08-03T11:19:50.055510Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"test_processed = preprocessingPipeline(test)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T11:20:05.823634Z","iopub.execute_input":"2022-08-03T11:20:05.824126Z","iopub.status.idle":"2022-08-03T11:20:06.846657Z","shell.execute_reply.started":"2022-08-03T11:20:05.824075Z","shell.execute_reply":"2022-08-03T11:20:06.845233Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# countvectorising\n\nX3 = v3.transform(test_processed['tokens_string'])\nX3_matrix = X3.toarray()\nX3_features = v3.get_feature_names_out()\n\ntrigrams_df = pd.DataFrame(X3_matrix, columns=list(X3_features))\n\n# Combine with training data\n\ntest_trigram = test_processed.merge(pd.DataFrame(X3_matrix, columns=list(X3_features)), left_index=True, right_index=True, suffixes=['x', None])\n\nX = test_trigram.drop(['text', 'cleaned_text', 'tokens', 'tokens_lem', 'tokens_string', 'id',], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T11:37:35.087685Z","iopub.execute_input":"2022-08-03T11:37:35.088133Z","iopub.status.idle":"2022-08-03T11:37:39.166215Z","shell.execute_reply.started":"2022-08-03T11:37:35.088096Z","shell.execute_reply":"2022-08-03T11:37:39.164850Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"X.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-03T11:39:22.816370Z","iopub.execute_input":"2022-08-03T11:39:22.816783Z","iopub.status.idle":"2022-08-03T11:39:23.018113Z","shell.execute_reply.started":"2022-08-03T11:39:22.816748Z","shell.execute_reply":"2022-08-03T11:39:23.016875Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"## Create Test Submission","metadata":{}},{"cell_type":"code","source":"# test\n\nresults = clf.predict(X)\n\npd.DataFrame({'id': test.loc[:, 'id'], 'target': results}).to_csv(\"submission.csv\", index=None)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T11:48:34.455637Z","iopub.execute_input":"2022-08-03T11:48:34.456135Z","iopub.status.idle":"2022-08-03T11:48:35.383443Z","shell.execute_reply.started":"2022-08-03T11:48:34.456093Z","shell.execute_reply":"2022-08-03T11:48:35.382067Z"},"trusted":true},"execution_count":57,"outputs":[]}]}