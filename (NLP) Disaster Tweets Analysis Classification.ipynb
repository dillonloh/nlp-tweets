{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Natural Language Processing of Disaster Tweets\n\nTwitter has become an important source of real-time information, given its ease of use and quick access. Users can quickly send out short 'tweets' to update on disasters that they are witnessing, which has led to many companies and monitoring agencies using it as a source of real-time information. However, since the vocabulary used in describing disasters is also used in unrelated contexts or as literary devices, these agencies need a way of checking if a tweet containing these words is describing an actual disaster.\n","metadata":{}},{"cell_type":"markdown","source":"# Exploring the Data\n\nWe have a training and testing dataset consisting of tweets, some of which are related to an actual disaster. The training set has 7503 datapoints, and the testing set has 3243. The features present in the training set are:\n\n1. keywords\n + a particular keyword from the tweet (may be blank)\n2. location\n + the location the tweet was sent from (may be blank)\n3. text\n + the text of the tweet\n4. target\n + this denotes whether a tweet is about a real disaster (1) or not (0)\n \nWe will begin by exploring our data and performing the eye-test on the contents of the tweets. We will also be performing n-gram analysis on our tweets after doing some preprocessing.","metadata":{}},{"cell_type":"code","source":"# import necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.naive_bayes import *\nfrom sklearn.decomposition import *\nfrom sklearn.svm import *\nfrom sklearn.neural_network import MLPClassifier\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\n\nfrom nltk.tokenize import *\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nfrom nltk.probability import FreqDist\n\nimport re\nimport string\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 100000)\npd.set_option('display.max_colwidth', None)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:42:21.901558Z","iopub.execute_input":"2022-08-06T17:42:21.902075Z","iopub.status.idle":"2022-08-06T17:42:24.858228Z","shell.execute_reply.started":"2022-08-06T17:42:21.901967Z","shell.execute_reply":"2022-08-06T17:42:24.857139Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# nltk data used for preprocessing\n\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:42:33.744961Z","iopub.execute_input":"2022-08-06T17:42:33.745511Z","iopub.status.idle":"2022-08-06T17:42:34.446107Z","shell.execute_reply.started":"2022-08-06T17:42:33.745477Z","shell.execute_reply":"2022-08-06T17:42:34.445199Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:42:50.577333Z","iopub.execute_input":"2022-08-06T17:42:50.577728Z","iopub.status.idle":"2022-08-06T17:42:50.627185Z","shell.execute_reply.started":"2022-08-06T17:42:50.577698Z","shell.execute_reply":"2022-08-06T17:42:50.626152Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:42:50.908073Z","iopub.execute_input":"2022-08-06T17:42:50.908461Z","iopub.status.idle":"2022-08-06T17:42:50.940408Z","shell.execute_reply.started":"2022-08-06T17:42:50.908429Z","shell.execute_reply":"2022-08-06T17:42:50.939089Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"NO_TRAIN_NON_DISASTER = train['target'].value_counts()[0]\nNO_TRAIN_DISASTER = train['target'].value_counts()[1]\n","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:42:51.144293Z","iopub.execute_input":"2022-08-06T17:42:51.144723Z","iopub.status.idle":"2022-08-06T17:42:51.152587Z","shell.execute_reply.started":"2022-08-06T17:42:51.144687Z","shell.execute_reply":"2022-08-06T17:42:51.151277Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"We note that about 25% of the data contains missing location data, while only about 0.6% is missing keyword data.\n\nSince location data is not something that can be immediately derived without the use of external datasets, we decide to set the missing values to a placeholder value of 'no_location'.\n\nFor the keyword columns, one possible way of filling in the missing values is by parsing the grammatical structure of the tweet and searching for the grammatical 'object' and using it as the keyword. However, there is no guarantee on the correct spelling of the words in the tweet, which would add unnecessary outliers to our keyword data. (e.g. 'goal' is spelt as goooooooaaaaaalll' in tweet 28). Similarly, the keyword in some tweets do not necessarily correspond to its object. For example, tweet 37 ('No way...I can't eat that shit') has 'shit' as an object, though one would likely agree that 'eat' would likely be the more useful/descriptive keyword here.\n\nWe will hence fill in the remaining missing values with a placeholder of 'no_keyword'.","metadata":{}},{"cell_type":"code","source":"# fill in nas with no_location, no_keyword\n\ntrain_processed = train.fillna(value={'keyword': 'no_keyword', 'location': 'no_location'}, )","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:42:51.864331Z","iopub.execute_input":"2022-08-06T17:42:51.864844Z","iopub.status.idle":"2022-08-06T17:42:51.874839Z","shell.execute_reply.started":"2022-08-06T17:42:51.864792Z","shell.execute_reply":"2022-08-06T17:42:51.873426Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_processed.info()","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:42:51.925034Z","iopub.execute_input":"2022-08-06T17:42:51.925639Z","iopub.status.idle":"2022-08-06T17:42:51.944361Z","shell.execute_reply.started":"2022-08-06T17:42:51.925599Z","shell.execute_reply":"2022-08-06T17:42:51.942852Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"We want to look through the top keywords and locations from our dataset.","metadata":{}},{"cell_type":"code","source":"train_processed[train_processed['target']==1]['keyword'].value_counts()[:20].plot.barh()\nplt.title('Top 20 keyword values (Disasters)')","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:42:52.023478Z","iopub.execute_input":"2022-08-06T17:42:52.023931Z","iopub.status.idle":"2022-08-06T17:42:52.498927Z","shell.execute_reply.started":"2022-08-06T17:42:52.023884Z","shell.execute_reply":"2022-08-06T17:42:52.497550Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_processed[train_processed['target']==0]['keyword'].value_counts()[:20].plot.barh()\nplt.title('Top 20 keyword values (Not Disasters)')","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:42:52.501566Z","iopub.execute_input":"2022-08-06T17:42:52.502059Z","iopub.status.idle":"2022-08-06T17:42:52.795850Z","shell.execute_reply.started":"2022-08-06T17:42:52.502015Z","shell.execute_reply":"2022-08-06T17:42:52.794953Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_processed[(train_processed['target'] == 0) & (train_processed['keyword'] == 'blizzard')].head()","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:42:52.797323Z","iopub.execute_input":"2022-08-06T17:42:52.798498Z","iopub.status.idle":"2022-08-06T17:42:52.817619Z","shell.execute_reply.started":"2022-08-06T17:42:52.798443Z","shell.execute_reply":"2022-08-06T17:42:52.816470Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Surprising, words like 'blizzard' and 'body bags' are part of the top 20 keyword values for non-disaster related tweets. To get some context on these types of tweets, we looked at several examples. We note that tweets which had 'blizzard', for example, was used to refer to the company Blizzard. This informs us that we cannot just use a single word as an indicator for whether a tweet is about a disaster or not. ","metadata":{}},{"cell_type":"code","source":"train_processed[train_processed['target']==1]['location'].value_counts()[1:11].plot.barh()\nplt.title('Top 10 location values (Disasters)\\n Excludes no_location')","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:42:52.820331Z","iopub.execute_input":"2022-08-06T17:42:52.821167Z","iopub.status.idle":"2022-08-06T17:42:53.052441Z","shell.execute_reply.started":"2022-08-06T17:42:52.821119Z","shell.execute_reply":"2022-08-06T17:42:53.051121Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_processed[train_processed['target']==0]['location'].value_counts()[1:11].plot.barh()\nplt.title('Top 10 location values (Not Disasters)\\nExcludes no_location')","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:42:53.055964Z","iopub.execute_input":"2022-08-06T17:42:53.057040Z","iopub.status.idle":"2022-08-06T17:42:53.289830Z","shell.execute_reply.started":"2022-08-06T17:42:53.056999Z","shell.execute_reply":"2022-08-06T17:42:53.288707Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"temp1 = train_processed[train_processed['target']==1]['location'].value_counts()[1:11]\ntemp2 = train_processed[train_processed['target']==0]['location'].value_counts().filter(items=list(temp1.index), axis=0)\n\ntemp = pd.DataFrame({'disaster': temp1, 'non_disaster': temp2})\ntemp = temp.div(temp.sum(axis=1), axis=0)\n\ntemp.plot.barh()\n\nplt.title('Location vs. whether a tweet is related to disaster or non-disaster')\nplt.xlabel('Percentage of total tweets')","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:42:53.291349Z","iopub.execute_input":"2022-08-06T17:42:53.291683Z","iopub.status.idle":"2022-08-06T17:42:53.564624Z","shell.execute_reply.started":"2022-08-06T17:42:53.291652Z","shell.execute_reply":"2022-08-06T17:42:53.563582Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"We see that tweets from certain locations have a higher proportions of tweets that are related to disasters than non-disasters. In particular, Mumbai, India, and Nigeria exhibit such behaviour. Locations like New York, however, are more likely to have non-disaster tweets than disaster tweets.","metadata":{}},{"cell_type":"markdown","source":"# Data Preprocessing\n\nNext, we will begin to preprocess our data and clean it up in preparation for feature learning. We will use the basic steps shared by in this [intro notebook by Parul Pandey](https://www.kaggle.com/parulpandey) to guide our preprocessing.\n\nThe techniques include:\n- Lower casing\n- Tokenisation\n- Stop words removal\n- Stemming\n- Lemmatization\n\n[API reference](https://www.nltk.org/api/nltk.html) and [guide to using with Pandas](https://www.kirenz.com/post/2021-12-11-text-mining-and-sentiment-analysis-with-nltk-and-pandas-in-python/text-mining-and-sentiment-analysis-with-nltk-and-pandas-in-python/)","metadata":{}},{"cell_type":"markdown","source":"## Lower-casing","metadata":{}},{"cell_type":"code","source":"# lower casing all texts\n\ntrain_processed['cleaned_text'] = train_processed['text'].str.lower()","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:42:53.566154Z","iopub.execute_input":"2022-08-06T17:42:53.566486Z","iopub.status.idle":"2022-08-06T17:42:53.576763Z","shell.execute_reply.started":"2022-08-06T17:42:53.566455Z","shell.execute_reply":"2022-08-06T17:42:53.575758Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_processed.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-08-06T17:42:53.578299Z","iopub.execute_input":"2022-08-06T17:42:53.578931Z","iopub.status.idle":"2022-08-06T17:42:53.591805Z","shell.execute_reply.started":"2022-08-06T17:42:53.578888Z","shell.execute_reply":"2022-08-06T17:42:53.590946Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Text cleaning\n\n**URL Removal**\n\nWe note from analysis that we did retroactively that there are URLs present in many tweets that link to external sites. As such, we will attempt to remove them so that common strings that are useless for us like 'http' will not show up in our data.\n\n**Hashtags removal**\n\nSimilarly, hashtags are common in tweets but do not have any special meaning for our use. Hence, we will remove them.\n\n**Remove special character's strings**\n\nWe will also remove special characters that appear in tweets. Some of these are due to encoding errors, leading to the appearance of non-alphanumeric strings. We remove them based on a list referenced from [here](https://deepnote.com/@pk/DeFi-DL-57c78dd7-2199-4952-9c0d-748f13156a1a).\n\n**Replace special characters encoding**\n\nSince some special characters like ampersands (&) are reserved in HTML, they are represented by certain strings in order to display them. We will replace them with the actual characters in our preprocessing.\n\n**Remove punctuation at start and end of tokens**\nSome words have a punctuation mark attached to them, likely due to \n\nFinally, we will also remove characters outside of the first 128 unicode characters (e.g. Û)","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    result = re.sub(r\"http\\S+\", \"\", text)\n    result = re.sub(r\"https\\S+\", \"\", result)\n    result = re.sub(r\"#\", \"\", result)\n\n    # remove special characters\n    result = re.sub(r\"\\x89Û_\", \"\", result)\n    result = re.sub(r\"\\x89ÛÒ\", \"\", result)\n    result = re.sub(r\"\\x89ÛÓ\", \"\", result)\n    result = re.sub(r\"\\x89ÛÏWhen\", \"When\", result)\n    result = re.sub(r\"\\x89ÛÏ\", \"\", result)\n    result = re.sub(r\"let\\x89Ûªs\", \"let's\", result)\n    result = re.sub(r\"\\x89Û÷\", \"\", result)\n    result = re.sub(r\"\\x89Ûª\", \"\", result)\n    result = re.sub(r\"\\x89Û\\x9d\", \"\", result)\n    result = re.sub(r\"å_\", \"\", result)\n    result = re.sub(r\"\\x89Û¢\", \"\", result)\n    result = re.sub(r\"\\x89Û¢åÊ\", \"\", result)\n    result = re.sub(r\"åÊ\", \"\", result)\n    result = re.sub(r\"åÈ\", \"\", result)\n    result = re.sub(r\"Ì©\", \"e\", result)\n    result = re.sub(r\"å¨\", \"\", result)\n    result = re.sub(r\"åÇ\", \"\", result)\n    result = re.sub(r\"åÀ\", \"\", result)\n    \n    # Character entity references\n    result = re.sub(r\"&gt;\", \">\", result)\n    result = re.sub(r\"&lt;\", \"<\", result)\n    result = re.sub(r\"&amp;\", \"&\", result)\n    \n    \n    # We will remove all chars not in Unicode 128 https://www.utf8-chartable.de/unicode-utf8-table.pl?number=128]\n    \n    result = ''.join([char for char in result if ord(char)<=128])\n    \n    return result\n\ntrain_processed['cleaned_text'] = train_processed['cleaned_text'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:42:53.594840Z","iopub.execute_input":"2022-08-06T17:42:53.595243Z","iopub.status.idle":"2022-08-06T17:42:53.927628Z","shell.execute_reply.started":"2022-08-06T17:42:53.595209Z","shell.execute_reply":"2022-08-06T17:42:53.926543Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Tokenisation","metadata":{}},{"cell_type":"code","source":"# Tokenisation via nltk\n\ntrain_processed['tokens'] = train_processed['cleaned_text'].apply(word_tokenize)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:42:53.928999Z","iopub.execute_input":"2022-08-06T17:42:53.929347Z","iopub.status.idle":"2022-08-06T17:42:55.369175Z","shell.execute_reply.started":"2022-08-06T17:42:53.929315Z","shell.execute_reply":"2022-08-06T17:42:55.367903Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_processed.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-08-06T17:42:55.370682Z","iopub.execute_input":"2022-08-06T17:42:55.371146Z","iopub.status.idle":"2022-08-06T17:42:55.388487Z","shell.execute_reply.started":"2022-08-06T17:42:55.371110Z","shell.execute_reply":"2022-08-06T17:42:55.387229Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Stop-words removal","metadata":{}},{"cell_type":"code","source":"# remove stop words using nltk\n\nstopwords = nltk.corpus.stopwords.words('english')\n\ntrain_processed['tokens'] = train_processed['tokens'].apply(lambda x: [item for item in x if item not in stopwords])","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:42:55.389794Z","iopub.execute_input":"2022-08-06T17:42:55.390144Z","iopub.status.idle":"2022-08-06T17:42:55.722006Z","shell.execute_reply.started":"2022-08-06T17:42:55.390114Z","shell.execute_reply":"2022-08-06T17:42:55.720929Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"train_processed.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:42:55.723708Z","iopub.execute_input":"2022-08-06T17:42:55.724622Z","iopub.status.idle":"2022-08-06T17:42:55.743948Z","shell.execute_reply.started":"2022-08-06T17:42:55.724579Z","shell.execute_reply":"2022-08-06T17:42:55.742977Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Remove punctuation tokens + remove punctuation from start/end of tokens.","metadata":{}},{"cell_type":"code","source":"# remove punctuation tokens\n\npuncs = [char for char in string.punctuation]\n    \ntrain_processed['tokens'] = train_processed['tokens'].apply(lambda x: [item for item in x if item not in puncs])\ntrain_processed['tokens'] = train_processed['tokens'].apply(lambda x: [item.strip(''.join(puncs)) for item in x])","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:42:55.745144Z","iopub.execute_input":"2022-08-06T17:42:55.745635Z","iopub.status.idle":"2022-08-06T17:42:55.868954Z","shell.execute_reply.started":"2022-08-06T17:42:55.745599Z","shell.execute_reply":"2022-08-06T17:42:55.867770Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train_processed.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:42:55.870344Z","iopub.execute_input":"2022-08-06T17:42:55.870696Z","iopub.status.idle":"2022-08-06T17:42:55.888320Z","shell.execute_reply.started":"2022-08-06T17:42:55.870664Z","shell.execute_reply":"2022-08-06T17:42:55.887092Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Lemmatize tokens","metadata":{}},{"cell_type":"code","source":"# lemmatise using wordnetlemmatise\n\nwnl = WordNetLemmatizer()\n\ntrain_processed['tokens_lem'] = train_processed['tokens'].apply(lambda x: [wnl.lemmatize(item) for item in x])\n","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:42:55.890441Z","iopub.execute_input":"2022-08-06T17:42:55.891338Z","iopub.status.idle":"2022-08-06T17:42:58.286430Z","shell.execute_reply.started":"2022-08-06T17:42:55.891264Z","shell.execute_reply":"2022-08-06T17:42:58.285454Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"train_processed.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-08-06T17:42:58.289629Z","iopub.execute_input":"2022-08-06T17:42:58.290335Z","iopub.status.idle":"2022-08-06T17:42:58.312381Z","shell.execute_reply.started":"2022-08-06T17:42:58.290282Z","shell.execute_reply":"2022-08-06T17:42:58.310792Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Encode categorical variables","metadata":{}},{"cell_type":"code","source":"# encode using sklearn's OrdinalEncoder\n\n# these encoders will be used for the training, validation, and test sets.\noe_location = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=6969)\noe_keyword = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=6969)\n\noe_location.fit(train_processed['location'].to_frame())\noe_keyword.fit(train_processed['keyword'].to_frame())\n\ntrain_processed['location'] = oe_location.transform(train_processed['location'].to_frame())\ntrain_processed['keyword'] = oe_keyword.transform(train_processed['keyword'].to_frame())","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:42:58.315151Z","iopub.execute_input":"2022-08-06T17:42:58.316042Z","iopub.status.idle":"2022-08-06T17:42:58.350198Z","shell.execute_reply.started":"2022-08-06T17:42:58.315996Z","shell.execute_reply":"2022-08-06T17:42:58.349168Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## Count Vectorise our dataset","metadata":{}},{"cell_type":"code","source":"# make the tokens into a string for our sklearn countvectoriser function\n\ndef combine_tokens(tokens):\n    string = ''\n    for token in tokens:\n        string += ' '\n        string += token\n    return string\n\ntrain_processed['tokens_string'] = train_processed['tokens_lem'].apply(combine_tokens)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:42:58.351412Z","iopub.execute_input":"2022-08-06T17:42:58.351752Z","iopub.status.idle":"2022-08-06T17:42:58.373373Z","shell.execute_reply.started":"2022-08-06T17:42:58.351720Z","shell.execute_reply":"2022-08-06T17:42:58.372237Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"train_processed.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:42:58.374821Z","iopub.execute_input":"2022-08-06T17:42:58.375974Z","iopub.status.idle":"2022-08-06T17:42:58.400917Z","shell.execute_reply.started":"2022-08-06T17:42:58.375935Z","shell.execute_reply":"2022-08-06T17:42:58.399762Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Unigrams (1-grams)","metadata":{}},{"cell_type":"code","source":"# Get counts of each unique token for each corpus.\n\nv1 = CountVectorizer()\n\nX1 = v1.fit_transform(train_processed['tokens_string'])\nX1_matrix = X1.toarray()\nX1_features = v1.get_feature_names_out()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-08-04T15:59:23.686079Z","iopub.execute_input":"2022-08-04T15:59:23.686409Z","iopub.status.idle":"2022-08-04T15:59:23.907225Z","shell.execute_reply.started":"2022-08-04T15:59:23.686373Z","shell.execute_reply":"2022-08-04T15:59:23.905508Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"unigrams_df = pd.DataFrame(X1_matrix, columns=list(X1_features))","metadata":{"execution":{"iopub.status.busy":"2022-08-04T15:59:23.911562Z","iopub.execute_input":"2022-08-04T15:59:23.912006Z","iopub.status.idle":"2022-08-04T15:59:23.919673Z","shell.execute_reply.started":"2022-08-04T15:59:23.911971Z","shell.execute_reply":"2022-08-04T15:59:23.918600Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# plot the top 10 most frequent unigrams\ntemp = unigrams_df.sum().sort_values(ascending=False)\n\nplt.figure(figsize=(12,10))\ntemp[:50].plot.barh()\nplt.title('Top 50 most frequent unigrams')","metadata":{"execution":{"iopub.status.busy":"2022-08-04T15:59:23.921320Z","iopub.execute_input":"2022-08-04T15:59:23.921851Z","iopub.status.idle":"2022-08-04T15:59:24.608839Z","shell.execute_reply.started":"2022-08-04T15:59:23.921814Z","shell.execute_reply":"2022-08-04T15:59:24.607718Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Combine with training data\n\ntrain_unigram = train_processed.merge(unigrams_df, left_index=True, right_index=True, suffixes=['x', None])\n\ntrain_unigram.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-08-04T15:59:24.610187Z","iopub.execute_input":"2022-08-04T15:59:24.610428Z","iopub.status.idle":"2022-08-04T15:59:26.910754Z","shell.execute_reply.started":"2022-08-04T15:59:24.610403Z","shell.execute_reply":"2022-08-04T15:59:26.909895Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# check most common unigrams of disaster and non-disaster related tweets\n\ntemp = train_unigram.groupby('targetx').sum().drop('idx', axis=1)\nindexes = list(unigrams_df.sum().sort_values(ascending=False).index)[:10]\n\nt1 = temp.iloc[0, :].loc[indexes]\nt2 = temp.iloc[1, :].loc[indexes]\n\nt3 = pd.DataFrame({'disaster': t2, 'non_disaster': t1})\nt3['disaster'] = t3['disaster'].div(NO_TRAIN_DISASTER)\nt3['non_disaster'] = t3['non_disaster'].div(NO_TRAIN_NON_DISASTER)\n\nt3.plot.barh()\nplt.title('Unigrams vs frequency in tweets(split by disaster vs non_disaster tweets)')","metadata":{"execution":{"iopub.status.busy":"2022-08-04T15:59:26.914154Z","iopub.execute_input":"2022-08-04T15:59:26.914690Z","iopub.status.idle":"2022-08-04T15:59:27.540056Z","shell.execute_reply.started":"2022-08-04T15:59:26.914658Z","shell.execute_reply":"2022-08-04T15:59:27.538574Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## Bigrams (2-grams)","metadata":{}},{"cell_type":"code","source":"# Get counts of each unique token for each corpus.\n\nv2 = CountVectorizer(ngram_range=(2, 2))\n\nX2 = v2.fit_transform(train_processed['tokens_string'])\nX2_matrix = X2.toarray()\nX2_features = v2.get_feature_names_out()","metadata":{"execution":{"iopub.status.busy":"2022-08-04T15:59:27.540727Z","iopub.status.idle":"2022-08-04T15:59:27.541035Z","shell.execute_reply.started":"2022-08-04T15:59:27.540886Z","shell.execute_reply":"2022-08-04T15:59:27.540901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bigrams_df = pd.DataFrame(X2_matrix, columns=list(X2_features))","metadata":{"execution":{"iopub.status.busy":"2022-08-04T15:59:27.542332Z","iopub.status.idle":"2022-08-04T15:59:27.542674Z","shell.execute_reply.started":"2022-08-04T15:59:27.542493Z","shell.execute_reply":"2022-08-04T15:59:27.542509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the top 10 most frequent bigrams\ntemp = bigrams_df.sum().sort_values(ascending=False)\n\ntemp[:10].plot.barh()\nplt.title('Top 10 most frequent bigrams')","metadata":{"execution":{"iopub.status.busy":"2022-08-04T15:59:27.543770Z","iopub.status.idle":"2022-08-04T15:59:27.544074Z","shell.execute_reply.started":"2022-08-04T15:59:27.543928Z","shell.execute_reply":"2022-08-04T15:59:27.543943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine with training data\n\ntrain_bigram = train_processed.merge(bigrams_df, left_index=True, right_index=True, suffixes=['x', None])\n\ntrain_bigram.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-08-04T15:59:27.544988Z","iopub.status.idle":"2022-08-04T15:59:27.545277Z","shell.execute_reply.started":"2022-08-04T15:59:27.545134Z","shell.execute_reply":"2022-08-04T15:59:27.545148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check most common bigrams of disaster and non-disaster related tweets\n\ntemp = train_bigram.groupby('target').sum().drop('id', axis=1)\nindexes = list(bigrams_df.sum().sort_values(ascending=False).index)[:10]\nt1 = temp.iloc[0, :].loc[indexes]\nt2 = temp.iloc[1, :].loc[indexes]\n\nt3 = pd.DataFrame({'disaster': t2, 'non_disaster': t1})\nt3['disaster'] = t3['disaster'].div(NO_TRAIN_DISASTER)\nt3['non_disaster'] = t3['non_disaster'].div(NO_TRAIN_NON_DISASTER)\n\nt3.plot.barh()\nplt.title('Bigrams vs normalised frequency in tweets(split by disaster vs non_disaster tweets)')","metadata":{"execution":{"iopub.status.busy":"2022-08-04T15:59:27.548233Z","iopub.status.idle":"2022-08-04T15:59:27.549599Z","shell.execute_reply.started":"2022-08-04T15:59:27.549395Z","shell.execute_reply":"2022-08-04T15:59:27.549413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trigrams (3-grams)","metadata":{}},{"cell_type":"code","source":"# Get counts of each unique token for each corpus.\n\nv3 = CountVectorizer(ngram_range=(3, 3))\n\nX3 = v3.fit(train_processed['tokens_string']).transform(train_processed['tokens_string'])\nX3_matrix = X3.toarray()\nX3_features = v3.get_feature_names_out()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trigrams_df = pd.DataFrame(X3_matrix, columns=list(X3_features))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the top 10 most frequent trigrams\ntemp = trigrams_df.sum().sort_values(ascending=False)\n\ntemp[:10].plot.barh()\nplt.title('Top 10 most frequent trigrams')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine with training data\n\ntrain_trigram = train_processed.merge(pd.DataFrame(X3_matrix, columns=list(X3_features)), left_index=True, right_index=True, suffixes=['x', None])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check most common trigrams of disaster and non-disaster related tweets\n\ntemp = train_trigram.groupby('target').sum().drop('id', axis=1)\nindexes = list(trigrams_df.sum().sort_values(ascending=False).index)[:10]\nt1 = temp.iloc[0, :].loc[indexes]\nt2 = temp.iloc[1, :].loc[indexes]\n\nt3 = pd.DataFrame({'disaster': t2, 'non_disaster': t1})\nt3['disaster'] = t3['disaster'].div(NO_TRAIN_DISASTER)\nt3['non_disaster'] = t3['non_disaster'].div(NO_TRAIN_NON_DISASTER)\n\nt3.plot.barh()\nplt.title('Trigrams vs normalised frequency in tweets(split by disaster vs non_disaster tweets)')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building a Trigram model\n\nWe choose a trigram model because of the top trigrams found in the dataset, the top 10 largely are related to disaster tweets -- more so than the unigram and bigrams. We do not go higher than this in order to maintain generality of our model.\n\n\nWe will attempt several methods.","metadata":{}},{"cell_type":"markdown","source":"## Should we TFIDF weight our tokens?\n\nSince common words that occur throughout the whole corpus do not offer much information in differentiating between different types of tweets (similar to stop words), we want to weigh it such that words which appear less frequently will have more weight than words that appear everywhere. To do so, we can apply Term Frequency-Inverse Document Frequency weighting. However, after experimentation, we note that this leads to worse results. Hence, we will not be using it.","metadata":{}},{"cell_type":"code","source":"# Get counts of each unique token for each corpus.\n\nv3 = CountVectorizer(ngram_range=(3, 3), binary=True)\n\nX3 = v3.fit(train_processed['tokens_string']).transform(train_processed['tokens_string'])\nX3_matrix = X3.toarray()\nX3_features = v3.get_feature_names_out()\n\ntrigrams_df = pd.DataFrame(X3_matrix, columns=list(X3_features))\n\n# Combine with training data\n\ntrain_trigram = train_processed.merge(pd.DataFrame(X3_matrix, columns=list(X3_features)), left_index=True, right_index=True, suffixes=['x', None])\n","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:43:24.909321Z","iopub.execute_input":"2022-08-06T17:43:24.909853Z","iopub.status.idle":"2022-08-06T17:43:31.869126Z","shell.execute_reply.started":"2022-08-06T17:43:24.909806Z","shell.execute_reply":"2022-08-06T17:43:31.867861Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"X = train_trigram.drop(['text', 'cleaned_text', 'tokens', 'tokens_lem', 'tokens_string', 'id', 'target'], axis=1)\ny = train_trigram.loc[:, 'target']","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:43:31.871237Z","iopub.execute_input":"2022-08-06T17:43:31.871619Z","iopub.status.idle":"2022-08-06T17:43:35.102728Z","shell.execute_reply.started":"2022-08-06T17:43:31.871586Z","shell.execute_reply":"2022-08-06T17:43:35.101742Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX = 0\ny = 0","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:43:35.104328Z","iopub.execute_input":"2022-08-06T17:43:35.105571Z","iopub.status.idle":"2022-08-06T17:43:37.349821Z","shell.execute_reply.started":"2022-08-06T17:43:35.105522Z","shell.execute_reply":"2022-08-06T17:43:37.348627Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-06T17:43:37.352208Z","iopub.execute_input":"2022-08-06T17:43:37.352644Z","iopub.status.idle":"2022-08-06T17:43:37.546613Z","shell.execute_reply.started":"2022-08-06T17:43:37.352603Z","shell.execute_reply":"2022-08-06T17:43:37.545741Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## Should we do Topic Modelling via LDA?\n\nWe note that since the number of unique tokens is not that high (<100000), we do not need to do dimensionality reduction. Experimentation also showed that LDA led to worse results.","metadata":{"_kg_hide-input":false}},{"cell_type":"markdown","source":"## Logistic Regression\n\nWe will use a logistic regression classification model with cross validation (3-folds). We chose this model due to the data being exceptionally high dimensional, which would make a Tree-based model work extremely slow.","metadata":{}},{"cell_type":"code","source":"clf = LogisticRegressionCV(cv=3, random_state=42, max_iter=3000, verbose=1).fit(X_train, y_train)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# validation\n\nclf.score(lda.transform(X_test), y_test)","metadata":{"execution":{"iopub.status.busy":"2022-08-04T16:25:52.220326Z","iopub.execute_input":"2022-08-04T16:25:52.220693Z","iopub.status.idle":"2022-08-04T16:25:52.237879Z","shell.execute_reply.started":"2022-08-04T16:25:52.220667Z","shell.execute_reply":"2022-08-04T16:25:52.236515Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"## Naive Bayes\n\nWe will also try a Naive Bayes method, as it is known to work well for document classification.\nhttps://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes","metadata":{}},{"cell_type":"code","source":"nb_gaussian = GaussianNB().fit(X_train, y_train)\nnb_categorical = CategoricalNB().fit(X_train, y_train)\nnb_complement = ComplementNB().fit(X_train, y_train)\nnb_multinomial = MultinomialNB().fit(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-04T16:23:30.673049Z","iopub.execute_input":"2022-08-04T16:23:30.673429Z","iopub.status.idle":"2022-08-04T16:23:46.931646Z","shell.execute_reply.started":"2022-08-04T16:23:30.673395Z","shell.execute_reply":"2022-08-04T16:23:46.930467Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"print('Gaussian NB:',nb_gaussian.score((X_test), y_test))\nprint('Complement NB:', nb_complement.score((X_test), y_test))\nprint('Multinomial NB:', nb_multinomial.score((X_test), y_test))","metadata":{"execution":{"iopub.status.busy":"2022-08-04T16:30:52.738548Z","iopub.execute_input":"2022-08-04T16:30:52.739200Z","iopub.status.idle":"2022-08-04T16:30:54.692028Z","shell.execute_reply.started":"2022-08-04T16:30:52.739173Z","shell.execute_reply":"2022-08-04T16:30:54.690181Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"## Support Vector Machine (SVM)\n\nLastly, we will try SVMs, as they are ['widely regarded as one of the best text classification algorithms'](https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568).","metadata":{}},{"cell_type":"code","source":"svc = SVC(verbose=1, max_iter=-1, random_state=42,).fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-08-04T16:23:13.774090Z","iopub.execute_input":"2022-08-04T16:23:13.774544Z","iopub.status.idle":"2022-08-04T16:23:16.503920Z","shell.execute_reply.started":"2022-08-04T16:23:13.774498Z","shell.execute_reply":"2022-08-04T16:23:16.501883Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"svc.score(X_test[:], y_test[:])","metadata":{"execution":{"iopub.status.busy":"2022-08-04T16:23:23.143941Z","iopub.execute_input":"2022-08-04T16:23:23.144295Z","iopub.status.idle":"2022-08-04T16:23:23.572807Z","shell.execute_reply.started":"2022-08-04T16:23:23.144270Z","shell.execute_reply":"2022-08-04T16:23:23.572016Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"## Neural Network (MLP)","metadata":{}},{"cell_type":"code","source":"# use sklearn's for quick testing\n\nnn = MLPClassifier(hidden_layer_sizes=(256, 16, ), random_state=42, verbose=1, max_iter=50, warm_start=True).fit(X_train, y_train)\n\nimport pickle\n\nwith open('pickle.pickle', 'wb') as f:\n    pickle.dump(nn.get_params(), f)","metadata":{"execution":{"iopub.status.busy":"2022-08-05T10:04:48.363356Z","iopub.execute_input":"2022-08-05T10:04:48.364598Z","iopub.status.idle":"2022-08-05T10:29:37.977563Z","shell.execute_reply.started":"2022-08-05T10:04:48.364506Z","shell.execute_reply":"2022-08-05T10:29:37.974320Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"nn.score(X_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2022-08-05T09:54:39.881232Z","iopub.execute_input":"2022-08-05T09:54:39.882936Z","iopub.status.idle":"2022-08-05T09:54:41.618827Z","shell.execute_reply.started":"2022-08-05T09:54:39.882868Z","shell.execute_reply":"2022-08-05T09:54:41.617121Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# Creating Test Submission\n\nNow that we have a basic model - though admittedly the validation score of 0.66 is not great - we can prepare our testing file and use the model to predict the target values.","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('../input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-08-06T18:34:10.097550Z","iopub.execute_input":"2022-08-06T18:34:10.098050Z","iopub.status.idle":"2022-08-06T18:34:10.128638Z","shell.execute_reply.started":"2022-08-06T18:34:10.098012Z","shell.execute_reply":"2022-08-06T18:34:10.127393Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"markdown","source":"## Preparing Test Data\n\nTo make things more concise, we will create a simple preprocessing pipeline. After that, we will count vectorise the tokens and merge the count matrix with our data.","metadata":{}},{"cell_type":"code","source":"def preprocessingPipeline(data):\n    \n    processed = data.fillna(value={'keyword': 'no_keyword', 'location': 'no_location'}, )\n    \n    # lower casing all texts\n    \n    processed['cleaned_text'] = processed['text'].str.lower()\n    \n    # remove urls and hashtags\n\n    processed['cleaned_text'] = processed['cleaned_text'].apply(clean_text)\n    # Tokenisation via nltk\n    \n    processed['tokens'] = processed['cleaned_text'].apply(word_tokenize)\n    \n    # remove stop words using nltk\n\n    stopwords = nltk.corpus.stopwords.words('english')\n    processed['tokens'] = processed['tokens'].apply(lambda x: [item for item in x if item not in stopwords])\n    \n    # remove punctuation tokens\n\n    puncs = [char for char in string.punctuation]\n    processed['tokens'] = processed['tokens'].apply(lambda x: [item for item in x if item not in puncs])\n    processed['tokens'] = processed['tokens'].apply(lambda x: [item.strip(''.join(puncs)) for item in x])\n    \n    \n    # lemmatise using wordnetlemmatise\n\n    wnl = WordNetLemmatizer()\n    processed['tokens_lem'] = processed['tokens'].apply(lambda x: [wnl.lemmatize(item) for item in x])\n\n    # make the tokens into a string for our sklearn countvectoriser function\n\n    processed['tokens_string'] = processed['tokens_lem'].apply(combine_tokens)\n    \n    # encode using training set trained OrdinalEncoders\n    \n    processed['location'] = oe_location.transform(processed['location'].to_frame())\n    processed['keyword'] = oe_keyword.transform(processed['keyword'].to_frame())\n    \n    return processed","metadata":{"execution":{"iopub.status.busy":"2022-08-06T18:34:12.014813Z","iopub.execute_input":"2022-08-06T18:34:12.015272Z","iopub.status.idle":"2022-08-06T18:34:12.028376Z","shell.execute_reply.started":"2022-08-06T18:34:12.015236Z","shell.execute_reply":"2022-08-06T18:34:12.027365Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"test_processed = preprocessingPipeline(test)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T18:34:30.926522Z","iopub.execute_input":"2022-08-06T18:34:30.926994Z","iopub.status.idle":"2022-08-06T18:34:32.069402Z","shell.execute_reply.started":"2022-08-06T18:34:30.926952Z","shell.execute_reply":"2022-08-06T18:34:32.068412Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"# countvectorising\n\nX3 = v3.transform(test_processed['tokens_string'])\nX3_matrix = X3.toarray()\nX3_features = v3.get_feature_names_out()\n\ntrigrams_df = pd.DataFrame(X3_matrix, columns=list(X3_features))\n\n# Combine with training data\n\ntest_trigram = test_processed.merge(pd.DataFrame(X3_matrix, columns=list(X3_features)), left_index=True, right_index=True, suffixes=['x', None])\n\nX = test_trigram.drop(['text', 'cleaned_text', 'tokens', 'tokens_lem', 'tokens_string', 'id',], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T18:34:35.762947Z","iopub.execute_input":"2022-08-06T18:34:35.763327Z","iopub.status.idle":"2022-08-06T18:34:40.171398Z","shell.execute_reply.started":"2022-08-06T18:34:35.763295Z","shell.execute_reply":"2022-08-06T18:34:40.170140Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"markdown","source":"## Create Test Submission","metadata":{}},{"cell_type":"code","source":"outputs = []\nfor i, row in enumerate(temp_loader, 0):\n    output = model(row[0])\n    if output > 0.5:\n        outputs.append([i, 1])\n    else:\n        outputs.append([i, 0])\n    ","metadata":{"execution":{"iopub.status.busy":"2022-08-06T18:45:12.294795Z","iopub.execute_input":"2022-08-06T18:45:12.295247Z","iopub.status.idle":"2022-08-06T18:45:17.896611Z","shell.execute_reply.started":"2022-08-06T18:45:12.295208Z","shell.execute_reply":"2022-08-06T18:45:17.895069Z"},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"code","source":"results = np.array(outputs)\n\npd.DataFrame({'id': results[:, 0], 'target': results[:, 1]}).to_csv('submission.csv', index=None)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T18:45:36.142309Z","iopub.execute_input":"2022-08-06T18:45:36.142764Z","iopub.status.idle":"2022-08-06T18:45:36.159600Z","shell.execute_reply.started":"2022-08-06T18:45:36.142730Z","shell.execute_reply":"2022-08-06T18:45:36.158636Z"},"trusted":true},"execution_count":151,"outputs":[]},{"cell_type":"code","source":"# test\n\nresults = nb_gaussian.predict(X)\n\npd.DataFrame({'id': test.loc[:, 'id'], 'target': results}).to_csv(\"submission.csv\", index=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Future Work\n\nBased on the already competitive validation score of the basic sklearn MLP neural network architecture, we can imagine that improving the NN's architecture would lead to results that are better than our current best method ('Naive Bayes'). In particular, Recurrent Neural Networks (RNN) are commonly used for text classification processes. We will attempt to implement this later on.","metadata":{}},{"cell_type":"code","source":"# build architecture\n\nclass TextClassifier(nn.Module):\n    def __init__(self):\n        \n        super().__init__()\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(45075, 100),\n            nn.ReLU(),\n            nn.Linear(100, 1),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        \n        result = self.classifier(x)\n        return result\n    \n    \n# model init\nmodel = TextClassifier()\n\n# validation using \n\nloss_function = nn.BCELoss()","metadata":{"execution":{"iopub.status.busy":"2022-08-06T18:31:21.981159Z","iopub.execute_input":"2022-08-06T18:31:21.981536Z","iopub.status.idle":"2022-08-06T18:31:22.024716Z","shell.execute_reply.started":"2022-08-06T18:31:21.981501Z","shell.execute_reply":"2022-08-06T18:31:22.023113Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"losses_epoch = []","metadata":{"execution":{"iopub.status.busy":"2022-08-06T18:31:22.026128Z","iopub.execute_input":"2022-08-06T18:31:22.026495Z","iopub.status.idle":"2022-08-06T18:31:22.057226Z","shell.execute_reply.started":"2022-08-06T18:31:22.026462Z","shell.execute_reply":"2022-08-06T18:31:22.055849Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"# load dataset\n\ndataset = TensorDataset(torch.Tensor(X_train.to_numpy()), torch.Tensor(y_train.to_numpy()).type(torch.LongTensor))\ntrain_loader = DataLoader(dataset, batch_size=32, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T18:31:19.729399Z","iopub.execute_input":"2022-08-06T18:31:19.730685Z","iopub.status.idle":"2022-08-06T18:31:21.979256Z","shell.execute_reply.started":"2022-08-06T18:31:19.730627Z","shell.execute_reply":"2022-08-06T18:31:21.978124Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.SGD(model.parameters(), lr=0.001,)\nepochs = 10\n\nfor epoch in range(epochs):\n    losses = []\n    for i, data in enumerate(train_loader, 0):\n\n        inputs, labels = data\n\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = loss_function(outputs, labels.unsqueeze(1).type(torch.FloatTensor))\n        loss.backward()\n        optimizer.step()\n    \n        # print statistics\n        losses.append(loss.detach())\n            \n    losses_epoch.append(losses)\n    l = sum(losses)/len(losses)\n    print('epoch [{}/{}]'.format(epoch + 1, epochs,), f'Loss: {l}')\n\nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2022-08-06T18:31:25.794358Z","iopub.execute_input":"2022-08-06T18:31:25.794798Z","iopub.status.idle":"2022-08-06T18:32:30.131198Z","shell.execute_reply.started":"2022-08-06T18:31:25.794762Z","shell.execute_reply":"2022-08-06T18:32:30.129887Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"# load dataset\n\ntemp_dataset = TensorDataset(torch.Tensor(X.to_numpy()))\ntemp_loader = DataLoader(temp_dataset, batch_size=1, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T18:45:08.178091Z","iopub.execute_input":"2022-08-06T18:45:08.178484Z","iopub.status.idle":"2022-08-06T18:45:09.313540Z","shell.execute_reply.started":"2022-08-06T18:45:08.178453Z","shell.execute_reply":"2022-08-06T18:45:09.312534Z"},"trusted":true},"execution_count":149,"outputs":[]}]}